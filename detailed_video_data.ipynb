{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0492f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "from functools import reduce\n",
    "from hashlib import md5\n",
    "import urllib.parse\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from retrying import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33dacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixinKeyEncTab = [\n",
    "    46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,\n",
    "    33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,\n",
    "    61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,\n",
    "    36, 20, 34, 44, 52\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec07b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMixinKey(orig: str):\n",
    "    '对 imgKey 和 subKey 进行字符顺序打乱编码'\n",
    "    return reduce(lambda s, i: s + orig[i], mixinKeyEncTab, '')[:32]\n",
    "\n",
    "def encWbi(params: dict, img_key: str, sub_key: str):\n",
    "    '为请求参数进行 wbi 签名'\n",
    "    mixin_key = getMixinKey(img_key + sub_key)\n",
    "    curr_time = round(time.time())\n",
    "    params['wts'] = curr_time                                   # 添加 wts 字段\n",
    "    params = dict(sorted(params.items()))                       # 按照 key 重排参数\n",
    "    # 过滤 value 中的 \"!'()*\" 字符\n",
    "    params = {\n",
    "        k : ''.join(filter(lambda chr: chr not in \"!'()*\", str(v)))\n",
    "        for k, v \n",
    "        in params.items()\n",
    "    }\n",
    "    query = urllib.parse.urlencode(params)                      # 序列化参数\n",
    "    wbi_sign = md5((query + mixin_key).encode()).hexdigest()    # 计算 w_rid\n",
    "    params['w_rid'] = wbi_sign\n",
    "    return params\n",
    "\n",
    "def getWbiKeys() -> tuple[str, str]:\n",
    "    '获取最新的 img_key 和 sub_key'\n",
    "    resp = requests.get('https://api.bilibili.com/x/web-interface/nav')\n",
    "    resp.raise_for_status()\n",
    "    json_content = resp.json()\n",
    "    img_url: str = json_content['data']['wbi_img']['img_url']\n",
    "    sub_url: str = json_content['data']['wbi_img']['sub_url']\n",
    "    img_key = img_url.rsplit('/', 1)[1].split('.')[0]\n",
    "    sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]\n",
    "    return img_key, sub_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7201b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(uid,pn):\n",
    "    img_key, sub_key = getWbiKeys()\n",
    "    signed_params = encWbi(\n",
    "        params={\n",
    "            'mid': uid,\n",
    "            'ps': 50,\n",
    "            'tid': 0,\n",
    "            'pn': pn,\n",
    "            'keyword': '',\n",
    "            'order': 'pubdate',\n",
    "            'platform': 'web',\n",
    "            'web_location': '1550101',\n",
    "            'order_avoided':'true'\n",
    "        },\n",
    "        img_key=img_key,\n",
    "        sub_key=sub_key\n",
    "    )\n",
    "    query = urllib.parse.urlencode(signed_params)\n",
    "    url = 'https://api.bilibili.com/x/space/wbi/arc/search?'+ query\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8ace05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data):\n",
    "    with open('f_video_list.json', 'a',encoding='utf-8') as f:\n",
    "        for info in data:\n",
    "            f.write(info + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ddeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop_max_attempt_number=10,wait_fixed=2000)\n",
    "def get_videos_list(mid):       \n",
    "    #首先进入第一页拿到该up主的页数        \n",
    "    pn = 1\n",
    "    url = get_url(mid,pn)\n",
    "    header = # your header\n",
    "    response = requests.get(url=url, headers=header).json()\n",
    "    if 'data' not in response or 'list' not in response['data']:\n",
    "        print('Invalid response format')\n",
    "        return\n",
    "    videos_count = response['data']['page']['count']\n",
    "    pages =int(videos_count/50) +1\n",
    "    #存储第一页视频\n",
    "    videoslist = response['data']['list']['vlist']\n",
    "    video_lst = []\n",
    "    for item in videoslist:\n",
    "        video_lst.append(json.dumps(item, ensure_ascii=False))\n",
    "    #分情况保存：第一页，最后一页，其余页\n",
    "    if mid == up_list[0]:\n",
    "        #先保存第一页内容\n",
    "        with open('f_video_list.json', 'a',encoding='utf-8') as f:\n",
    "                f.write(\"[\"+ video_lst[0] +\",\")\n",
    "                video_lst.remove(video_lst[0])\n",
    "        save_file(video_lst)\n",
    "        #接下来保存其他页数\n",
    "        pn = 2\n",
    "        while pn <= pages:\n",
    "            url = get_url(mid,pn)\n",
    "            response = requests.get(url=url, headers=header).json()\n",
    "            videoslist = response['data']['list']['vlist']\n",
    "            video_lst = []\n",
    "            for item in videoslist:\n",
    "                video_lst.append(json.dumps(item, ensure_ascii=False))\n",
    "            save_file(video_lst)\n",
    "            pn += 1\n",
    "            time.sleep(3)\n",
    "    elif mid == up_list[len(up_list)-1]:\n",
    "        if pages == 1:\n",
    "            index_last = len(video_lst)-1\n",
    "            last_elem = video_lst[index_last]\n",
    "            video_lst.remove(last_elem)\n",
    "            save_file(video_lst)\n",
    "            with open('f_video_list.json','a',encoding='utf-8') as f:\n",
    "                    f.write(last_elem +\"]\")\n",
    "        else:\n",
    "            save_file(video_lst)\n",
    "            pn = 2\n",
    "            while pn <= pages:\n",
    "                if pn == pages:\n",
    "                    url = get_url(mid,pn)\n",
    "                    response = requests.get(url=url, headers=header).json()\n",
    "                    videoslist = response['data']['list']['vlist']\n",
    "                    video_lst = []\n",
    "                    for item in videoslist:\n",
    "                        video_lst.append(json.dumps(item, ensure_ascii=False))\n",
    "                    index_last = len(video_lst)-1\n",
    "                    last_elem = video_lst[index_last]\n",
    "                    video_lst.remove(last_elem)\n",
    "                    save_file(video_lst)\n",
    "                    with open('f_video_list.json','a',encoding='utf-8') as f:\n",
    "                            f.write(last_elem +\"]\")\n",
    "                else:     \n",
    "                    url = get_url(mid,pn)\n",
    "                    response = requests.get(url=url, headers=header).json()\n",
    "                    videoslist = response['data']['list']['vlist']\n",
    "                    video_lst = []\n",
    "                    for item in videoslist:\n",
    "                        video_lst.append(json.dumps(item, ensure_ascii=False))\n",
    "                    save_file(video_lst)\n",
    "                    time.sleep(3)\n",
    "                pn += 1\n",
    "    else:\n",
    "        save_file(video_lst)\n",
    "        pn = 2\n",
    "        while pn <= pages:\n",
    "            url = get_url(mid,pn)\n",
    "            response = requests.get(url=url, headers=header).json()\n",
    "            videoslist = response['data']['list']['vlist']\n",
    "            video_lst = []\n",
    "            for item in videoslist:\n",
    "                video_lst.append(json.dumps(item, ensure_ascii=False))\n",
    "            save_file(video_lst)\n",
    "            pn += 1\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0764be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入源文件中合作的up_uid\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "up_list = np.load('up_list.npy').tolist()\n",
    "np.load.__defaults__=(None, False, True, 'ASCII') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i in up_list:\n",
    "    get_videos_list(i)\n",
    "    n += 1\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "337d1597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('f_video_list.json','r',encoding='utf-8') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c82f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_header = ['bvid', 'mid', 'description', 'author', 'pic', 'created', 'play', 'length', 'title','copyright', 'is_pay', 'hide_click', 'is_charging_arc', 'aid', 'meta', 'vt', 'is_live_playback', 'video_review', 'is_steins_gate', 'enable_vt', 'comment', 'attribute', 'is_union_video', 'is_avoided', 'subtitle', 'review', 'typeid']\n",
    "with open('T_video_list_csv.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    dictWriter = csv.DictWriter(f, csv_header)\n",
    "    dictWriter.writeheader()\n",
    "    dictWriter.writerows(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cd60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('T_video_list_csv.csv', dtype={'play': str,'title': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e861a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['pic','description','subtitle','title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d0a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_video_list.csv\",index=False,encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
